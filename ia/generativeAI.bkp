// servicos/ia/generative-ai.js (ou modules/generativeAI.js)

/**
 * M√≥dulo de IA Generativa
 * Gera respostas em tempo real, naturais e contextuais usando ApiFreeLLM.com
 */

const axios = require('axios'); // Necess√°rio para fazer requisi√ß√µes HTTP para o ApiFreeLLM

class GenerativeAI {
    constructor(config, twitchClient, memoryManager = null) { // Adicionei memoryManager aqui
        this.config = config;
        this.twitchClient = twitchClient;
        this.memoryManager = memoryManager; // Armazena a inst√¢ncia do memoryManager

        // Configura√ß√£o para ApiFreeLLM.com
        // N√£o √© necess√°ria chave API para o acesso gratuito e ilimitado
        this.apiFreeLlmUrl = "https://apifreellm.com/api/chat"; // Endpoint do ApiFreeLLM

        this.conversationHistory = [];
        this.gameplayContext = {
            currentGame: 'Jogo n√£o detectado',
            recentEvents: [],
            stats: {}
        };
        this.intensity = this.config.ai?.intensity || 0.5;
        this.isActive = false;
        this.responseQueue = [];
        this.lastResponseTime = 0;
        this.minResponseInterval = 60000; // 60 segundos entre respostas autom√°ticas

        // O prompt de sistema ser√° carregado do memoryManager
        this.systemPrompt = "Voc√™ √© um bot de Twitch prestativo e divertido."; // Fallback inicial

        this.personality = {
            style: 'divertido e emp√°tico',
            traits: ['criativo', 'encorajador', 'humor√≠stico', 'respeitoso'],
            restrictions: ['sem spam', 'sem conte√∫do ofensivo', 'sem spoilers']
        };
    }

    /**
     * Carrega o prompt de sistema do banco de dados via memoryManager.
     */
    async loadSystemPrompt() {
        if (!this.memoryManager) {
            console.warn('MemoryManager n√£o configurado para IA Generativa. Usando prompt padr√£o.');
            return;
        }
        try {
            const dbPrompt = await this.memoryManager.getConfig('ai_system_prompt');
            if (dbPrompt) {
                this.systemPrompt = dbPrompt;
                console.log('Prompt de sistema da IA carregado do banco de dados.');
            } else {
                console.log('Nenhum prompt de sistema encontrado no banco de dados para IA. Usando prompt padr√£o.');
            }
        } catch (error) {
            console.error('Erro ao carregar prompt de sistema da IA do banco de dados:', error);
            console.log('Usando prompt de sistema padr√£o devido ao erro.');
        }
    }

    /**
     * Ativa a IA generativa
     */
    async activate() { // Tornar async para aguardar loadSystemPrompt
        if (this.isActive) {
            console.log('IA Generativa j√° est√° ativa');
            return;
        }

        console.log('Ativando IA Generativa...');
        await this.loadSystemPrompt(); // Carrega o prompt ao ativar
        this.isActive = true;

        // Processar fila de respostas a cada 5 segundos
        this.responseProcessor = setInterval(() => {
            this.processResponseQueue();
        }, 5000);

        console.log('IA Generativa ativada com o prompt:');
        console.log(this.systemPrompt); // Log do prompt carregado
    }

    /**
     * Desativa a IA generativa
     */
    deactivate() {
        if (!this.isActive) {
            console.log('IA Generativa j√° est√° inativa');
            return;
        }

        console.log('Desativando IA Generativa...');
        this.isActive = false;

        if (this.responseProcessor) {
            clearInterval(this.responseProcessor);
            this.responseProcessor = null;
        }

        console.log('IA Generativa desativada');
    }

    /**
     * Processa mensagens do chat para poss√≠vel resposta da IA
     */
    async processChatMessage(channel, userstate, message) {
        if (!this.isActive) return;

        // Adicionar mensagem ao hist√≥rico
        this.addToConversationHistory(userstate.username, message);

        // Verificar se deve responder
        if (this.shouldRespond(message, userstate)) {
            await this._generateResponseForChat(channel, userstate, message);
        }
    }

    /**
     * Processa eventos de gameplay para contexto
     */
    processGameplayEvent(event) {
        if (!this.isActive) return;

        // Adicionar evento ao contexto
        this.gameplayContext.recentEvents.push({
            ...event,
            timestamp: Date.now()
        });

        // Manter apenas os √∫ltimos 10 eventos
        if (this.gameplayContext.recentEvents.length > 10) {
            this.gameplayContext.recentEvents.shift();
        }

        // Salvar evento na mem√≥ria (se memoryManager estiver configurado)
        if (this.memoryManager) {
            this.memoryManager.saveGameplayEvent(
                event.type,
                event.context,
                event.intensity,
                this.gameplayContext.currentGame
            );
        }

        // Gerar coment√°rio sobre evento importante
        if (event.intensity > 0.7) {
            this.queueGameplayResponse(event);
        }
    }

    /**
     * Atualiza estat√≠sticas de gameplay
     */
    updateGameplayStats(stats) {
        this.gameplayContext.stats = { ...stats };
    }

    /**
     * Atualiza jogo atual
     */
    updateCurrentGame(gameName) {
        this.gameplayContext.currentGame = gameName || 'Jogo n√£o detectado';
    }

    /**
     * Determina se deve responder a uma mensagem de chat
     * (L√≥gica do cavalo.txt, adaptada para ser mais robusta)
     */
    shouldRespond(message, userstate) {
        // N√£o responder a comandos
        if (message.startsWith(this.config.bot?.prefix || '!')) return false;

        // N√£o responder muito frequentemente
        const timeSinceLastResponse = Date.now() - this.lastResponseTime;
        if (timeSinceLastResponse < this.minResponseInterval) return false;

        // Responder se mencionado (nome do bot ou 'ia')
        const botUsername = this.config.twitch.username.toLowerCase();
        const messageLower = message.toLowerCase();
        if (messageLower.includes(botUsername) || messageLower.includes('ia')) {
            return true;
        }

        // Responder baseado na intensidade configurada (chance aleat√≥ria)
        const responseChance = this.intensity * 0.1; // 0-10% chance baseado na intensidade
        return Math.random() < responseChance;
    }

    /**
     * Gera resposta para mensagens de chat usando ApiFreeLLM
     * (Combina generateResponse do cavalo.txt com a chamada ApiFreeLLM do copia.txt)
     */
    async _generateResponseForChat(channel, userstate, message) {
        try {
            const context = this._buildContext(userstate, message);
            const fullPrompt = this._buildPromptForApiFreeLLM(message, {
                eventType: 'chat_mention', // Ou 'chat_general'
                username: userstate.username,
                game: context.currentGame,
                recentEvents: context.recentEvents,
                stats: context.stats,
                conversationHistory: context.conversationHistory
            });

            const aiResponse = await this._callApiFreeLLM(fullPrompt);

            // Filtrar e limpar a resposta
            const filteredResponse = this._filterAndCleanResponse(aiResponse);
            if (filteredResponse) {
                this.twitchClient.say(channel, `@${userstate.username} ${filteredResponse}`);
                this.lastResponseTime = Date.now();
                console.log(`IA respondeu no chat: @${userstate.username} ${filteredResponse}`);

                // Salvar intera√ß√£o com resposta na mem√≥ria (se memoryManager estiver configurado)
                if (this.memoryManager) {
                    this.memoryManager.saveChatInteraction(userstate.username, message, filteredResponse, channel);
                }
            }

        } catch (error) {
            console.error("Erro ao gerar resposta da IA (ApiFreeLLM):", error);
            // Opcional: enviar uma mensagem de erro gen√©rica para o chat
            // this.twitchClient.say(channel, `Desculpe, @${userstate.username}, tive um problema ao gerar a resposta.`);
        }
    }

    /**
     * Adiciona resposta de gameplay √† fila
     */
    queueGameplayResponse(event) {
        const response = {
            type: 'gameplay',
            event: event,
            timestamp: Date.now()
        };
        this.responseQueue.push(response);
    }

    /**
     * Processa fila de respostas
     */
    async processResponseQueue() {
        if (this.responseQueue.length === 0) return;

        const response = this.responseQueue.shift();

        // Verificar se n√£o passou muito tempo desde que o evento foi enfileirado
        const timeDiff = Date.now() - response.timestamp;
        if (timeDiff > 60000) { // Ignorar se passou mais de 1 minuto
            console.log('Evento na fila ignorado por tempo limite.');
            return;
        }

        if (response.type === 'gameplay') {
            await this._generateGameplayComment(response.event);
        }
    }

    /**
     * Gera coment√°rio sobre evento de gameplay
     * (Combina generateGameplayComment do cavalo.txt com generateGameplayPrompt e shouldRespond do copia.txt)
     */
    async _generateGameplayComment(event) {
        // Verifica se deve responder com base na intensidade e cooldown
        // A intensidade do evento √© usada para decidir se vale a pena comentar
        const eventIntensity = event.intensity || 0.5;
        const threshold = 1 - this.intensity; // Quanto maior a intensidade da IA, menor o threshold para responder
        if (eventIntensity < threshold) {
            return; // N√£o comenta se o evento n√£o for "intenso" o suficiente para a IA
        }

        // Verifica o intervalo m√≠nimo entre respostas autom√°ticas
        const timeSinceLastResponse = Date.now() - this.lastResponseTime;
        if (timeSinceLastResponse < this.minResponseInterval) {
            return;
        }
        try {
            const promptBase = this._generateGameplayPrompt(event);
            const fullPrompt = this._buildPromptForApiFreeLLM(promptBase, {
                eventType: event.type,
                game: this.gameplayContext.currentGame
            });

            const comment = await this._callApiFreeLLM(fullPrompt);
            const filteredComment = this._filterAndCleanResponse(comment);

            if (filteredComment) {
                // Enviar para todos os canais configurados
                this.config.twitch.channels.forEach(channel => {
                    this.twitchClient.say(channel, filteredComment);
                });
                this.lastResponseTime = Date.now(); // Atualiza o tempo da √∫ltima resposta
                console.log(`IA comentou gameplay: ${filteredComment}`);
            }
        } catch (error) {
            console.error("Erro ao gerar coment√°rio de gameplay (ApiFreeLLM):", error);
        }
    }

    /**
     * Constr√≥i contexto para a IA
     * (Do cavalo.txt)
     */
    _buildContext(userstate, message) {
        return {
            username: userstate.username,
            message: message,
            currentGame: this.gameplayContext.currentGame,
            recentEvents: this.gameplayContext.recentEvents.slice(-3), // √öltimos 3 eventos
            stats: this.gameplayContext.stats,
            conversationHistory: this.conversationHistory.slice(-5) // √öltimas 5 intera√ß√µes
        };
    }

    /**
     * Constr√≥i o prompt completo para a IA do ApiFreeLLM, incluindo personalidade e contexto.
     * (Baseado no buildPrompt do copia.txt, adaptado para usar o contexto do cavalo.txt)
     * @param {string} userPrompt - A mensagem original do usu√°rio ou evento.
     * @param {object} context - Contexto adicional (tipo de evento, jogo atual, etc.).
     * @returns {string} O prompt formatado para ser enviado √† IA.
     */
    _buildPromptForApiFreeLLM(userPrompt, context) {
        const gameContext = context.game || this.gameplayContext.currentGame;
        const eventType = context.eventType || 'geral';

        // Usa o systemPrompt carregado do banco de dados
        let systemPrompt = this.systemPrompt;

        if (gameContext !== 'Jogo n√£o detectado') {
            systemPrompt += ` O jogo atual √© ${gameContext}.`;
        }

        // Adiciona contexto espec√≠fico de evento, se aplic√°vel
        if (eventType === 'kill') {
            systemPrompt += ` O jogador acabou de eliminar um inimigo. Comemore essa conquista!`;
        } else if (eventType === 'death') {
            systemPrompt += ` O jogador foi eliminado. Seja encorajador e positivo.`;
        } else if (eventType === 'win') {
            systemPrompt += ` O jogador venceu! Comemore essa vit√≥ria incr√≠vel!`;
        } else if (eventType === 'combo') {
            systemPrompt += ` O jogador fez um combo espetacular!`;
        } else if (eventType === 'chat_mention' && context.username) {
            systemPrompt += ` O usu√°rio ${context.username} mencionou voc√™ no chat. Responda diretamente a ele.`;
        }

        // Adiciona hist√≥rico de conversa para manter algum contexto
        const history = this.conversationHistory
            .map(entry => `Usu√°rio: ${entry.username}: ${entry.message}`) // Usando o formato do cavalo.txt
            .join('\n');

        let finalPrompt = systemPrompt;
        if (history) {
            finalPrompt += `\n\nHist√≥rico recente:\n${history}\n\n`;
        }
        finalPrompt += `Mensagem/Evento: "${userPrompt}"\n\nSua resposta:`;

        return finalPrompt;
    }

    /**
     * Gera um prompt espec√≠fico para eventos de gameplay.
     * (Do copia.txt)
     * @param {object} eventData - Dados do evento de gameplay.
     * @returns {string} Um prompt aleat√≥rio para o evento.
     */
    _generateGameplayPrompt(eventData) {
        const prompts = {
            kill: [
                "Que elimina√ß√£o incr√≠vel!",
                "Jogada perfeita!",
                "Dominando o jogo!",
                "Que precis√£o!"
            ],
            death: [
                "N√£o desista, voc√™ consegue!",
                "Pr√≥xima vez vai dar certo!",
                "Faz parte do jogo!",
                "Volta mais forte!"
            ],
            win: [
                "VIT√ìRIA √âPICA!",
                "Que partida incr√≠vel!",
                "Domina√ß√£o total!",
                "Jogou demais!"
            ],
            combo: [
                "Que combo espetacular!",
                "Sequ√™ncia perfeita!",
                "Habilidade pura!",
                "Impressionante!"
            ]
        };
        const eventPrompts = prompts[eventData.type] || ["Que jogada!"];
        return eventPrompts[Math.floor(Math.random() * eventPrompts.length)];
    }

    /**
     * Faz a chamada HTTP para a API do ApiFreeLLM.
     * @param {string} fullPrompt - O prompt completo a ser enviado para a IA.
     * @returns {Promise<string|null>} A resposta gerada pela IA ou null em caso de erro.
     */
    async _callApiFreeLLM(fullPrompt) {
        try {
            const payload = {
                message: fullPrompt // ApiFreeLLM.com espera um campo 'message'
            };

            const headers = {
                'Content-Type': 'application/json'
            };

            const response = await axios.post(this.apiFreeLlmUrl, payload, { headers, timeout: 15000 });
            if (response.data && response.data.response) { // ApiFreeLLM.com retorna no campo 'response'
                return response.data.response.trim();
            }
            return null;
        } catch (error) {
            console.error('Erro ao chamar a API do ApiFreeLLM:', error.response?.status || error.message);
            if (error.response && error.response.data) {
                console.error('Detalhes do erro da API:', error.response.data);
            }
            throw new Error(`Falha na comunica√ß√£o com a IA: ${error.message}`);
        }
    }

    /**
     * Filtra e limpa a resposta da IA.
     * (Combina filterResponse do cavalo.txt e cleanResponse do copia.txt)
     */
    _filterAndCleanResponse(response) {
        if (!response) return null;

        let cleanedText = response;

        // 1. Verificar palavras banidas (do cavalo.txt)
        const bannedWords = this.config.filters?.bannedWords || [];
        const lowerResponse = cleanedText.toLowerCase();

        for (const word of bannedWords) {
            if (lowerResponse.includes(word.toLowerCase())) {
                console.log('Resposta filtrada por conter palavra banida');
                return null; // Retorna nulo se contiver palavra banida
            }
        }

        // 2. Remover quebras de linha excessivas (do copia.txt)
        cleanedText = cleanedText.replace(/\n+/g, ' ').trim();

        // 3. Limitar tamanho (do copia.txt, ajustado para ser mais restritivo para chat)
        if (cleanedText.length > 400) {
            cleanedText = cleanedText.substring(0, 397) + '...';
        }
        // 4. Remover caracteres especiais problem√°ticos (do copia.txt)
        // Mant√©m letras, n√∫meros, espa√ßos, pontua√ß√£o b√°sica e alguns emojis comuns de jogo/rea√ß√£o
        cleanedText = cleanedText.replace(/[^\w\s\u00C0-\u017F!?.,;:()üî•üí™üéØ‚ö°ÔøΩÔøΩ]/g, '');

        return cleanedText.trim();
    }

    /**
     * Adiciona mensagem ao hist√≥rico de conversa
     * (Do cavalo.txt)
     */
    addToConversationHistory(username, message) {
        this.conversationHistory.push({
            username: username,
            message: message,
            timestamp: Date.now()
        });
        // Manter apenas as √∫ltimas 20 mensagens
        if (this.conversationHistory.length > 20) {
            this.conversationHistory.shift();
        }
    }

    /**
     * Atualiza intensidade da IA
     * (Do cavalo.txt)
     */
    updateIntensity(intensity) {
        this.intensity = Math.max(0, Math.min(1, intensity));
        console.log(`Intensidade da IA atualizada para: ${this.intensity}`);
    }

    /**
     * Limpa hist√≥rico e contexto
     * (Do cavalo.txt)
     */
    reset() {
        this.conversationHistory = [];
        this.gameplayContext.recentEvents = [];
        this.responseQueue = [];
        console.log('Contexto da IA resetado');
    }

    /**
     * Obt√©m estat√≠sticas da IA
     * (Do cavalo.txt)
     */
    getStats() {
        return {
            isActive: this.isActive,
            intensity: this.intensity,
            conversationHistoryLength: this.conversationHistory.length,
            recentEventsLength: this.gameplayContext.recentEvents.length,
            queueLength: this.responseQueue.length,
            systemPrompt: this.systemPrompt // Inclui o prompt atual nos stats
        };
    }
}

module.exports = GenerativeAI;